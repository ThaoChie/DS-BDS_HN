{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70158a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang cạo dữ liệu từ trang 1: https://batdongsan.com.vn/ban-nha-dat-ha-noi\n",
      "Đang cạo dữ liệu từ trang 2: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p2\n",
      "Đang cạo dữ liệu từ trang 3: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p3\n",
      "Đang cạo dữ liệu từ trang 4: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p4\n",
      "Đang cạo dữ liệu từ trang 5: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p5\n",
      "Đang cạo dữ liệu từ trang 6: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p6\n",
      "Đang cạo dữ liệu từ trang 7: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p7\n",
      "Đang cạo dữ liệu từ trang 8: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p8\n",
      "Đang cạo dữ liệu từ trang 9: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p9\n",
      "Lỗi khi truy cập trang https://batdongsan.com.vn/ban-nha-dat-ha-noi/p9: 403 Client Error: Forbidden for url: https://batdongsan.com.vn/ban-nha-dat-ha-noi/p9\n",
      "Đã cạo xong tất cả các trang.\n",
      "\n",
      "Bắt đầu cạo dữ liệu chi tiết của từng sản phẩm...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    180\u001b[39m     base_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://batdongsan.com.vn/ban-nha-dat-ha-noi\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     all_scraped_data = \u001b[43mscrape_all_pages_and_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m all_scraped_data:\n\u001b[32m    185\u001b[39m         save_to_csv(all_scraped_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mscrape_all_pages_and_details\u001b[39m\u001b[34m(base_url)\u001b[39m\n\u001b[32m    148\u001b[39m processed_data = []\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m detailed_data:\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# Cập nhật giá và diện tích sau khi cạo chi tiết\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     row[\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mparse_number_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprice_raw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtỷ|triệu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m     row[\u001b[33m'\u001b[39m\u001b[33mArea\u001b[39m\u001b[33m'\u001b[39m] = parse_number_from_text(row.get(\u001b[33m'\u001b[39m\u001b[33marea_raw\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mm2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    153\u001b[39m     processed_data.append(row)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mparse_number_from_text\u001b[39m\u001b[34m(text, unit)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m):\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: float() argument must be a string or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Cấu hình Headers và các đường dẫn\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "OUTPUT_CSV_FILE = 'batdongsan_listings.csv'\n",
    "\n",
    "def sane_sleep():\n",
    "    time.sleep(1)\n",
    "\n",
    "def parse_number_from_text(text: str, unit: str):\n",
    "    \"\"\"Trích xuất giá trị số từ chuỗi văn bản.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    s = text.lower().replace('.', '').replace(',', '.').strip()\n",
    "    m = re.search(r'([0-9\\.]+)\\s*' + unit, s)\n",
    "    if m:\n",
    "        try:\n",
    "            return float(m.group(1))\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def scrape_list_page(url: str) -> List[Dict]:\n",
    "    \"\"\"Cạo dữ liệu cơ bản từ một trang danh sách.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi khi truy cập trang {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    product_items = soup.find_all('div', class_='re__card-info')\n",
    "    \n",
    "    if not product_items:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    for item in product_items:\n",
    "        title_tag = item.find('a', class_='re__card-title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "        \n",
    "        price_tag = item.find('span', class_='re__card-config-price')\n",
    "        price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
    "        \n",
    "        area_tag = item.find('span', class_='re__card-config-area')\n",
    "        area = area_tag.get_text(strip=True) if area_tag else 'N/A'\n",
    "\n",
    "        url_tag = item.find('a', class_='re__card-title', href=True)\n",
    "        detail_url = f\"https://batdongsan.com.vn{url_tag['href']}\" if url_tag and url_tag['href'].startswith('/') else url_tag['href'] if url_tag else 'N/A'\n",
    "\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'price_raw': price,\n",
    "            'area_raw': area,\n",
    "            'detail_url': detail_url,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def scrape_detail_page(listing: Dict) -> Dict:\n",
    "    \"\"\"Cạo dữ liệu chi tiết từ trang của một bất động sản.\"\"\"\n",
    "    detail_url = listing.get('detail_url')\n",
    "    if not detail_url or detail_url == 'N/A':\n",
    "        return listing\n",
    "\n",
    "    try:\n",
    "        response = requests.get(detail_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        sane_sleep()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Lỗi khi truy cập trang chi tiết {detail_url}: {e}\")\n",
    "        return listing\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    detailed_data = {\n",
    "        'House Direction': None, 'Balcony Direction': None,\n",
    "        'Bedrooms': None, 'Toilets': None, 'Legits': None,\n",
    "        'Furnitures': None, 'Floors': None, 'Facade': None,\n",
    "        'Entrance': None, 'City': None, 'District': None,\n",
    "        'Ward': None, 'Street': None,\n",
    "    }\n",
    "\n",
    "    info_rows = soup.find_all('div', class_='re__pr-specs-content')\n",
    "    for row in info_rows:\n",
    "        key_tag = row.find('span', class_='re__pr-specs-label')\n",
    "        value_tag = row.find('span', class_='re__pr-specs-value')\n",
    "        \n",
    "        if key_tag and value_tag:\n",
    "            key = key_tag.get_text(strip=True).replace(':', '')\n",
    "            value = value_tag.get_text(strip=True)\n",
    "\n",
    "            if 'Hướng nhà' in key: detailed_data['House Direction'] = value\n",
    "            elif 'Hướng ban công' in key: detailed_data['Balcony Direction'] = value\n",
    "            elif 'Số phòng ngủ' in key: detailed_data['Bedrooms'] = value\n",
    "            elif 'Số toilet' in key: detailed_data['Toilets'] = value\n",
    "            elif 'Pháp lý' in key: detailed_data['Legits'] = value\n",
    "            elif 'Nội thất' in key: detailed_data['Furnitures'] = value\n",
    "            elif 'Số tầng' in key: detailed_data['Floors'] = value\n",
    "            elif 'Mặt tiền' in key: detailed_data['Facade'] = value\n",
    "            elif 'Đường' in key: detailed_data['Street'] = value\n",
    "\n",
    "    address_tag = soup.find('div', class_='re__pr-short-info-content')\n",
    "    if address_tag:\n",
    "        address_parts = [part.strip() for part in address_tag.get_text(strip=True).split(',') if part.strip()]\n",
    "        if len(address_parts) >= 1: detailed_data['Street'] = address_parts[0]\n",
    "        if len(address_parts) >= 2: detailed_data['Ward'] = address_parts[-3]\n",
    "        if len(address_parts) >= 3: detailed_data['District'] = address_parts[-2]\n",
    "        if len(address_parts) >= 4: detailed_data['City'] = address_parts[-1]\n",
    "\n",
    "    listing.update(detailed_data)\n",
    "    return listing\n",
    "\n",
    "def scrape_all_pages_and_details(base_url: str):\n",
    "    \"\"\"Cạo toàn bộ dữ liệu từ tất cả các trang và các trang chi tiết.\"\"\"\n",
    "    all_data = []\n",
    "    page_number = 1\n",
    "    \n",
    "    while True:\n",
    "        url = base_url if page_number == 1 else f\"{base_url}/p{page_number}\"\n",
    "        print(f\"Đang cạo dữ liệu từ trang {page_number}: {url}\")\n",
    "        \n",
    "        page_data = scrape_list_page(url)\n",
    "        if not page_data:\n",
    "            print(\"Đã cạo xong tất cả các trang.\")\n",
    "            break\n",
    "        \n",
    "        all_data.extend(page_data)\n",
    "        page_number += 1\n",
    "        sane_sleep()\n",
    "    \n",
    "    print(\"\\nBắt đầu cạo dữ liệu chi tiết của từng sản phẩm...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(scrape_detail_page, item): item for item in all_data}\n",
    "        detailed_data = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "    processed_data = []\n",
    "    for row in detailed_data:\n",
    "        # Cập nhật giá và diện tích sau khi cạo chi tiết\n",
    "        row['Price'] = parse_number_from_text(row.get('price_raw'), 'tỷ|triệu')\n",
    "        row['Area'] = parse_number_from_text(row.get('area_raw'), 'm2')\n",
    "        processed_data.append(row)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def save_to_csv(data: List[Dict]):\n",
    "    \"\"\"Lưu danh sách dict vào file CSV.\"\"\"\n",
    "    if not data:\n",
    "        print(\"Không có dữ liệu để lưu.\")\n",
    "        return\n",
    "\n",
    "    columns = [\n",
    "        'House Direction', 'Balcony Direction', 'Bedrooms', 'Toilets', \n",
    "        'Legits', 'Furnitures', 'Floors', 'Facade', 'Entrance', \n",
    "        'City', 'District', 'Ward', 'Street', 'Area', 'Price'\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df.reindex(columns=columns)\n",
    "    \n",
    "    df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(f\"\\nĐã lưu thành công {len(data)} bản ghi vào file '{OUTPUT_CSV_FILE}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    base_url = \"https://batdongsan.com.vn/ban-nha-dat-ha-noi\"\n",
    "    \n",
    "    all_scraped_data = scrape_all_pages_and_details(base_url)\n",
    "    \n",
    "    if all_scraped_data:\n",
    "        save_to_csv(all_scraped_data)\n",
    "    else:\n",
    "        print(\"Không thể cạo dữ liệu.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
