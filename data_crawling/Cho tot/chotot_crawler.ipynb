{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14276a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import date\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from lxml import etree\n",
    "import json\n",
    "import platform\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a106d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.home_page = \"https://batdongsan.com.vn/nha-dat-ban-ha-noi\"\n",
    "        self.root_url = \"https://batdongsan.com.vn\" # Thống nhất dùng root_url\n",
    "        self.page = 1\n",
    "    \n",
    "    def get_pages(self, page_source):\n",
    "        bs = BeautifulSoup(page_source, 'html.parser')\n",
    "        page = etree.HTML(str(bs))\n",
    "        links = [i.getchildren()[0].get(\"href\") for i in page.xpath(\"\"\"//*[@id=\"product-lists-web\"]/div\"\"\")]\n",
    "        links = [link for link in links if link is not None]\n",
    "        return [self.root_url + link for link in links]\n",
    "    \n",
    "    def next_page(self):\n",
    "        self.page += 1\n",
    "        return self.home_page + f\"/p{self.page}\"\n",
    "    \n",
    "    def gather(self, page_source):\n",
    "        data = {}\n",
    "        bs = BeautifulSoup(page_source, 'html.parser')\n",
    "        page = etree.HTML(str(bs))\n",
    "        \n",
    "        #get address\n",
    "        address = page.xpath(\"\"\"//*[@id=\"product-detail-web\"]/span\"\"\")[0].text\n",
    "        address = [i.strip() for i in address.split(\",\")[::-1]]\n",
    "        data[\"address\"] = address\n",
    "        \n",
    "        #get general description\n",
    "        descrip = \"\".join(page.xpath(\"\"\"//*[@id=\"product-detail-web\"]/div[2]\"\"\")[0].getchildren()[1].itertext())\n",
    "        data[\"gen_descrip\"] = descrip\n",
    "        \n",
    "        #get detailed description\n",
    "        try: \n",
    "            des_elements = page.xpath(\"\"\"//*[@id=\"product-detail-web\"]/div[3]/div/div\"\"\")[0].getchildren()\n",
    "            for feature in des_elements:\n",
    "                key = feature.getchildren()[1].text\n",
    "                value = feature.getchildren()[2].text\n",
    "                data[key] = value\n",
    "        except IndexError:\n",
    "            pass \n",
    "\n",
    "        #other information\n",
    "        oelements = bs.find_all(\"div\", {\"class\": \"re__pr-short-info-item js__pr-config-item\"})\n",
    "        try:\n",
    "            for feature in oelements:\n",
    "                feature = feature.find_all(\"span\")\n",
    "                key = feature[0].text\n",
    "                value = feature[1].text\n",
    "                data[key] = value\n",
    "            if \"Mã tin\" in data:\n",
    "                data[\"id\"] = \"batdongsan_com_vn_\" + str(data[\"Mã tin\"])\n",
    "            else:\n",
    "                data[\"id\"] = \"batdongsan_com_vn_\" + str(time.time())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            return data\n",
    "        return data \n",
    "    \n",
    "    def run(self, start, num_of_pages):\n",
    "        # create webdriver in a robust way: prefer platform-specific provided Edge driver (Windows),\n",
    "        # otherwise try to start Edge, then Chrome as a fallback.\n",
    "        driver = None\n",
    "        try:\n",
    "            # Windows-specific explicit path (only use if file exists)\n",
    "            if platform.system().lower().startswith(\"win\"):\n",
    "                edge_path = r\"/Users/thaochie/Desktop/bds_crawling/data_crawling/edgedriver_win64/msedgedriver.exe\"\n",
    "                if os.path.exists(edge_path):\n",
    "                    service = webdriver.edge.service.Service(edge_path)\n",
    "                    driver = webdriver.Edge(service=service)\n",
    "                else:\n",
    "                    # try default Edge\n",
    "                    driver = webdriver.Edge()\n",
    "            else:\n",
    "                # non-Windows: try Edge first, then Chrome\n",
    "                try:\n",
    "                    driver = webdriver.Edge()\n",
    "                except Exception:\n",
    "                    driver = webdriver.Chrome()\n",
    "        except Exception as e:\n",
    "            # surface useful error and stop early\n",
    "            print(\"Failed to start a webdriver. Make sure a driver is installed and on PATH.\")\n",
    "            print(repr(e))\n",
    "            raise\n",
    "\n",
    "        url = self.home_page\n",
    "        self.page = 1\n",
    "        if start != 1:\n",
    "            self.page = start\n",
    "            url = self.home_page+f\"/p{self.page}\"\n",
    "        # ensure data directory exists\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        if os.path.exists(r\"data/index.json\"):\n",
    "            with open(r\"data/index.json\") as f:\n",
    "                index = set(json.load(f))\n",
    "        else:\n",
    "            index = set()\n",
    "            \n",
    "        with open(\"log.txt\", \"w\") as f:\n",
    "            pass\n",
    "        \n",
    "        dataset = []\n",
    "        for i in range(num_of_pages):\n",
    "            num = 10\n",
    "            pages = []\n",
    "            while num > 0:\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    time.sleep(0.05)\n",
    "                    pages = self.get_pages(driver.page_source)\n",
    "                    driver.delete_all_cookies()\n",
    "                    break\n",
    "                except:\n",
    "                    num -= 1\n",
    "            if num <= 0:\n",
    "                url = self.next_page()\n",
    "                continue\n",
    "            for page in tqdm(pages):\n",
    "                num = 10\n",
    "                while num > 0:\n",
    "                    try:\n",
    "                        driver.get(page)\n",
    "                        time.sleep(0.05)\n",
    "                        data = self.gather(driver.page_source)\n",
    "                        # guard against missing id key\n",
    "                        rec_id = data.get(\"id\") if isinstance(data, dict) else None\n",
    "                        if not rec_id:\n",
    "                            # log and skip records without id\n",
    "                            with open(\"log.txt\", \"a\") as f:\n",
    "                                f.write(f\"Skipped (no id): {page}\\n\")\n",
    "                        else:\n",
    "                            if rec_id not in index:\n",
    "                                dataset.append(data)\n",
    "                                index.add(rec_id)\n",
    "                        with open(\"log.txt\", \"a\") as f:\n",
    "                            f.write(\"Success:\" + page +\"\\n\")\n",
    "                        driver.delete_all_cookies()\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        with open(\"log.txt\", \"a\") as f:\n",
    "                            f.write(f\"Excution{10-num}: {page}\"+\"\\n\") \n",
    "                            f.write(traceback.format_exc()+\"\\n\")\n",
    "                        driver.delete_all_cookies()\n",
    "                        num -= 1\n",
    "            url = self.next_page()\n",
    "        if len(dataset) != 0:\n",
    "            df = pd.DataFrame(dataset)\n",
    "            df.to_csv(f\"data/{start}_{start+num_of_pages}_{time.time()}.csv\", index=False)\n",
    "        with open(\"data/index.json\", \"w\") as f:\n",
    "            json.dump(list(index), f)\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except Exception:\n",
    "            try:\n",
    "                driver.close()\n",
    "            except Exception:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "craw = Crawler()\n",
    "\n",
    "for i in tqdm(range(599,2670)):\n",
    "    craw.run(i, 1)\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
